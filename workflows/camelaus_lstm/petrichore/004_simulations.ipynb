{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Union, Optional, Dict, List, Tuple\n",
    "import pickle\n",
    "import xarray as xr\n",
    "from neuralhydrology.utils.config import Config\n",
    "from neuralhydrology.evaluation.evaluate import start_evaluation\n",
    "import pandas as pd\n",
    "\n",
    "def eval_run(run_dir: Path, period: str, epoch: Optional[int] = None, gpu: Optional[int] = None, data_dir: Optional[str] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Start evaluating a trained model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    run_dir : Path\n",
    "        Path to the run directory.\n",
    "    period : {'train', 'validation', 'test'}\n",
    "        The period to evaluate.\n",
    "    epoch : int, optional\n",
    "        Define a specific epoch to use. By default, the weights of the last epoch are used.\n",
    "    gpu : int, optional\n",
    "        GPU id to use. Will override config argument 'device'. A value less than zero indicates CPU.\n",
    "    data_dir : str, optional\n",
    "        Directory containing the data. If provided, it updates the 'data_dir' in the configuration.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame containing evaluation results.\n",
    "    \"\"\"\n",
    "    config = Config(run_dir / \"config.yml\")\n",
    "\n",
    "    if data_dir is not None:\n",
    "        config.update_config({'data_dir': data_dir})\n",
    "\n",
    "    config.update_config({'run_dir': run_dir})\n",
    "\n",
    "    # check if a GPU has been specified as a command line argument. If yes, overwrite config\n",
    "    if gpu is not None and gpu >= 0:\n",
    "        config.device = f\"cuda:{gpu}\"\n",
    "    if gpu is not None and gpu < 0:\n",
    "        config.device = \"cpu\"\n",
    "\n",
    "    return start_evaluation(cfg=config, run_dir=run_dir, epoch=epoch, period=period)\n",
    "\n",
    "def cached_eval_run(run_dir: Path, period: str, data_dir: Optional[str] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load cached data or generate and save if not found.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    run_dir : Path\n",
    "        Path to the run directory.\n",
    "    period : {'train', 'validation', 'test'}\n",
    "        The period to load or generate data for.\n",
    "    data_dir : str, optional\n",
    "        Directory containing the data. If provided, it updates the 'data_dir' in the configuration.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame containing cached or generated data.\n",
    "    \"\"\"\n",
    "    postprocess_dir = Path(run_dir, \"postprocess\")\n",
    "    postprocess_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    cache_file = postprocess_dir / f\"{period}_cached_data.pkl\"\n",
    "\n",
    "    if cache_file.is_file():\n",
    "        print(f\"Loading cached data for {period} from {cache_file}\")\n",
    "        with open(cache_file, 'rb') as file:\n",
    "            return pickle.load(file)\n",
    "    else:\n",
    "        print(f\"Cached data for {period} not found. Generating and saving...\")\n",
    "        data = eval_run(run_dir=run_dir, period=period, data_dir=data_dir)\n",
    "        with open(cache_file, 'wb') as file:\n",
    "            pickle.dump(data, file)\n",
    "        return data\n",
    "\n",
    "def combine_datasets(results: Dict[int, Dict[str, Dict[str, Union[str, pd.DataFrame]]]]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Combine datasets from multiple folds.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    results : Dict[int, Dict[str, Dict[str, Union[str, pd.DataFrame]]]]\n",
    "        Dictionary containing results for different folds.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Combined DataFrame.\n",
    "    \"\"\"\n",
    "    basins = [basin for fold in results.values() for basin in fold]\n",
    "    data_values = [entry['1D']['xr'] for fold in results.values() for entry in fold.values()]\n",
    "    combined_dataset = xr.concat(data_values, dim='basin').assign_coords(basin=basins)\n",
    "    return combined_dataset.to_dataframe()\n",
    "\n",
    "def create_pivot_tables(df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Create pivot tables for simulated and observed streamflow.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame containing the streamflow data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[pd.DataFrame, pd.DataFrame]\n",
    "        Tuple containing pivot tables for simulated and observed streamflow.\n",
    "    \"\"\"\n",
    "    pivot_table_simulated = df.pivot_table(values='streamflow_mmd_sim', index='date', columns='basin')\n",
    "    pivot_table_observed = df.pivot_table(values='streamflow_mmd_obs', index='date', columns='basin')\n",
    "    return pivot_table_simulated, pivot_table_observed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cached data for test not found. Generating and saving...\n",
      "# Evaluation: 100%|██████████| 111/111 [08:37<00:00,  4.66s/it]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/sho108/projects/neuralhydrology/workflows/camelaus_lstm/petrichore/runs/spatial_twofold_0_pe_2501_181026/postprocess/test_cached_data.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mfor\u001b[39;00m fold \u001b[39min\u001b[39;00m folds:\n\u001b[1;32m      9\u001b[0m     run_dir \u001b[39m=\u001b[39m Path(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m/Users/sho108/projects/neuralhydrology/workflows/camelaus_lstm/petrichore/runs/spatial_twofold_\u001b[39m\u001b[39m{\u001b[39;00mfold\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mpredictors\u001b[39m}\u001b[39;00m\u001b[39m_2501_181026\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m     result[fold] \u001b[39m=\u001b[39m load_cached_data(run_dir, period, \u001b[39m'\u001b[39;49m\u001b[39m/Users/sho108/Desktop/z/Data/CAMELS_AUS\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     12\u001b[0m \u001b[39m# Extract basins and corresponding data values from the result dictionary\u001b[39;00m\n\u001b[1;32m     13\u001b[0m df \u001b[39m=\u001b[39m combine_datasets(result)\n",
      "Cell \u001b[0;32mIn[58], line 73\u001b[0m, in \u001b[0;36mload_cached_data\u001b[0;34m(run_dir, period, data_dir)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCached data for \u001b[39m\u001b[39m{\u001b[39;00mperiod\u001b[39m}\u001b[39;00m\u001b[39m not found. Generating and saving...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     72\u001b[0m data \u001b[39m=\u001b[39m eval_run(run_dir\u001b[39m=\u001b[39mrun_dir, period\u001b[39m=\u001b[39mperiod, data_dir\u001b[39m=\u001b[39mdata_dir)\n\u001b[0;32m---> 73\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(cache_file, \u001b[39m'\u001b[39;49m\u001b[39mwb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m file:\n\u001b[1;32m     74\u001b[0m     pickle\u001b[39m.\u001b[39mdump(data, file)\n\u001b[1;32m     75\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/projects/neuralhydrology/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[1;32m    304\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[0;32m--> 310\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/sho108/projects/neuralhydrology/workflows/camelaus_lstm/petrichore/runs/spatial_twofold_0_pe_2501_181026/postprocess/test_cached_data.pkl'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set parameters\n",
    "predictors = 'pe'\n",
    "period = \"test\"\n",
    "folds = [0, 1]\n",
    "bp = Path('/Users/sho108/projects/neuralhydrology/workflows/camelaus_lstm/petrichore/runs/')\n",
    "\n",
    "result = {\n",
    "    fold: cached_eval_run(\n",
    "        run_dir=bp/f'spatial_twofold_{fold}_{predictors}_2501_181026',\n",
    "        period=period,\n",
    "        data_dir='/Users/sho108/Desktop/z/Data/CAMELS_AUS'\n",
    "    )\n",
    "    for fold in folds\n",
    "}\n",
    "\n",
    "\n",
    "# Extract basins and corresponding data values from the result dictionary\n",
    "df = combine_datasets(result)\n",
    "\n",
    "# Create pivot tables for simulated and observed streamflow\n",
    "pivot_table_simulated, pivot_table_observed = create_pivot_tables(df)\n",
    "\n",
    "# Display the resulting DataFrames\n",
    "pivot_table_simulated.head(), pivot_table_observed.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/datasets/work/d61-coastal-forecasting-wp3/work/sho108/neuralhydrology/workflows/camelaus_lstm/runs/spatial_twofold_0_e_2501_181026/train_data\n",
    "/Users/sho108/projects/neuralhydrology/workflows/camelaus_lstm/petrichore/runs/spatial_twofold_0_e_2501_181026/train_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
