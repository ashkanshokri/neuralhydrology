{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralhydrology.nh_run import start_run, eval_run, continue_run\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from neuralhydrology.utils.file_system_operation import get_latest_touched_directory\n",
    "from neuralhydrology.training.train import start_training\n",
    "from neuralhydrology.utils.config import Config\n",
    "\n",
    "def get_predictor_list(s):\n",
    "    perdictor_dict = {\n",
    "    'p' : [\"precipitation_AWAP\"],\n",
    "    'e' : [\"et_morton_point_SILO\"],\n",
    "    'pe': [\"precipitation_AWAP\", \"et_morton_point_SILO\"]\n",
    "    }\n",
    "    return perdictor_dict[s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-25 10:34:08,250: Logging to /Users/sho108/projects/neuralhydrology/workflows/camelaus_lstm/runs/spatial_twofold_0_p_2501_103408/output.log initialized.\n",
      "2024-01-25 10:34:08,250: ### Folder structure created at /Users/sho108/projects/neuralhydrology/workflows/camelaus_lstm/runs/spatial_twofold_0_p_2501_103408\n",
      "2024-01-25 10:34:08,251: ### Run configurations for spatial_twofold_0_p\n",
      "2024-01-25 10:34:08,251: experiment_name: spatial_twofold_0_p\n",
      "2024-01-25 10:34:08,252: train_basin_file: basins/twofoldsplit/fold_0_train.txt\n",
      "2024-01-25 10:34:08,252: validation_basin_file: basins/twofoldsplit/fold_0_test.txt\n",
      "2024-01-25 10:34:08,252: test_basin_file: basins/twofoldsplit/fold_0_test.txt\n",
      "2024-01-25 10:34:08,253: train_start_date: 1981-01-01 00:00:00\n",
      "2024-01-25 10:34:08,253: train_end_date: 2018-12-31 00:00:00\n",
      "2024-01-25 10:34:08,254: validation_start_date: 1981-01-01 00:00:00\n",
      "2024-01-25 10:34:08,254: validation_end_date: 2018-12-31 00:00:00\n",
      "2024-01-25 10:34:08,254: test_start_date: 1981-01-01 00:00:00\n",
      "2024-01-25 10:34:08,255: test_end_date: 2018-12-31 00:00:00\n",
      "2024-01-25 10:34:08,255: device: cpu\n",
      "2024-01-25 10:34:08,255: validate_every: 5\n",
      "2024-01-25 10:34:08,256: validate_n_random_basins: 10\n",
      "2024-01-25 10:34:08,256: metrics: ['NSE', 'MSE', 'RMSE', 'KGE', 'Alpha-NSE', 'Pearson-r', 'Beta-KGE', 'Beta-NSE', 'FHV', 'FMS', 'FLV', 'Peak-Timing', 'Missed-Peaks', 'Peak-MAPE']\n",
      "2024-01-25 10:34:08,256: model: cudalstm\n",
      "2024-01-25 10:34:08,257: head: regression\n",
      "2024-01-25 10:34:08,257: output_activation: linear\n",
      "2024-01-25 10:34:08,258: hidden_size: 20\n",
      "2024-01-25 10:34:08,258: initial_forget_bias: 3\n",
      "2024-01-25 10:34:08,258: output_dropout: 0.4\n",
      "2024-01-25 10:34:08,259: optimizer: Adam\n",
      "2024-01-25 10:34:08,259: loss: MSE\n",
      "2024-01-25 10:34:08,259: learning_rate: {0: 0.01, 15: 0.005}\n",
      "2024-01-25 10:34:08,260: batch_size: 1024\n",
      "2024-01-25 10:34:08,260: epochs: 30\n",
      "2024-01-25 10:34:08,260: clip_gradient_norm: 1\n",
      "2024-01-25 10:34:08,260: predict_last_n: 1\n",
      "2024-01-25 10:34:08,261: seq_length: 365\n",
      "2024-01-25 10:34:08,261: num_workers: 8\n",
      "2024-01-25 10:34:08,261: log_interval: 5\n",
      "2024-01-25 10:34:08,261: log_tensorboard: True\n",
      "2024-01-25 10:34:08,262: log_n_figures: 1\n",
      "2024-01-25 10:34:08,262: save_weights_every: 1\n",
      "2024-01-25 10:34:08,262: dataset: camels_aus\n",
      "2024-01-25 10:34:08,262: data_dir: /Users/sho108/Desktop/z/Data/CAMELS_AUS\n",
      "2024-01-25 10:34:08,263: dynamic_inputs: ['precipitation_AWAP']\n",
      "2024-01-25 10:34:08,263: target_variables: ['streamflow_mmd']\n",
      "2024-01-25 10:34:08,263: clip_targets_to_zero: ['streamflow_mmd']\n",
      "2024-01-25 10:34:08,264: number_of_basins: 111\n",
      "2024-01-25 10:34:08,264: run_dir: /Users/sho108/projects/neuralhydrology/workflows/camelaus_lstm/runs/spatial_twofold_0_p_2501_103408\n",
      "2024-01-25 10:34:08,264: train_dir: /Users/sho108/projects/neuralhydrology/workflows/camelaus_lstm/runs/spatial_twofold_0_p_2501_103408/train_data\n",
      "2024-01-25 10:34:08,265: img_log_dir: /Users/sho108/projects/neuralhydrology/workflows/camelaus_lstm/runs/spatial_twofold_0_p_2501_103408/img_log\n",
      "2024-01-25 10:34:08,267: ### Device cpu will be used for training\n",
      "2024-01-25 10:34:08,268: Loading basin data into xarray data set.\n",
      "100%|██████████| 111/111 [04:25<00:00,  2.39s/it]\n",
      "2024-01-25 10:38:33,803: Create lookup table and convert to pytorch tensor\n",
      "100%|██████████| 111/111 [00:04<00:00, 23.07it/s]\n",
      "# Epoch 1: 100%|██████████| 1293/1293 [10:35<00:00,  2.03it/s, Loss: 0.0356]\n",
      "2024-01-25 10:49:14,490: Epoch 1 average loss: avg_loss: 0.24461, avg_total_loss: 0.24461\n",
      "# Epoch 2: 100%|██████████| 1293/1293 [10:37<00:00,  2.03it/s, Loss: 0.1700]\n",
      "2024-01-25 10:59:51,989: Epoch 2 average loss: avg_loss: 0.18957, avg_total_loss: 0.18957\n",
      "# Epoch 3: 100%|██████████| 1293/1293 [10:36<00:00,  2.03it/s, Loss: 0.2164]\n",
      "2024-01-25 11:10:28,647: Epoch 3 average loss: avg_loss: 0.18608, avg_total_loss: 0.18608\n",
      "# Epoch 4: 100%|██████████| 1293/1293 [10:34<00:00,  2.04it/s, Loss: 0.5626]\n",
      "2024-01-25 11:21:02,871: Epoch 4 average loss: avg_loss: 0.17910, avg_total_loss: 0.17910\n",
      "# Epoch 5: 100%|██████████| 1293/1293 [10:33<00:00,  2.04it/s, Loss: 0.0411]\n",
      "2024-01-25 11:31:35,942: Epoch 5 average loss: avg_loss: 0.17966, avg_total_loss: 0.17966\n",
      "# Validation: 100%|██████████| 10/10 [01:08<00:00,  6.80s/it]\n",
      "2024-01-25 11:32:44,263: Epoch 5 average validation loss: 0.06189 -- Median validation metrics: avg_loss: 0.06189, NSE: 0.55370, MSE: 0.78904, RMSE: 0.87675, KGE: 0.47079, Alpha-NSE: 0.84694, Pearson-r: 0.80627, Beta-KGE: 0.74197, Beta-NSE: -0.04017, FHV: -17.97985, FMS: 16.02620, FLV: -0.18251, Peak-Timing: 0.35667, Missed-Peaks: 0.34330, Peak-MAPE: 61.17925\n",
      "# Epoch 6:  41%|████      | 530/1293 [04:49<06:56,  1.83it/s, Loss: 0.1126] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 41\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[39mfor\u001b[39;00m predictors \u001b[39min\u001b[39;00m [\u001b[39m'\u001b[39m\u001b[39mp\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mpe\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39me\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[1;32m     40\u001b[0m     \u001b[39mfor\u001b[39;00m fold \u001b[39min\u001b[39;00m [\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m]:\n\u001b[0;32m---> 41\u001b[0m         _ \u001b[39m=\u001b[39m train_and_evaluate(fold, predictors, config_file)\n",
      "Cell \u001b[0;32mIn[5], line 19\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(fold, predictors, config_file)\u001b[0m\n\u001b[1;32m     16\u001b[0m cfg\u001b[39m.\u001b[39mdevice \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcuda:0\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     18\u001b[0m \u001b[39m# Start training\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m start_training(cfg)\n\u001b[1;32m     21\u001b[0m \u001b[39m# Get the latest run directory\u001b[39;00m\n\u001b[1;32m     22\u001b[0m run_dir \u001b[39m=\u001b[39m get_latest_touched_directory(\u001b[39m'\u001b[39m\u001b[39mruns\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/projects/neuralhydrology/neuralhydrology/training/train.py:20\u001b[0m, in \u001b[0;36mstart_training\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnknown head \u001b[39m\u001b[39m{\u001b[39;00mcfg\u001b[39m.\u001b[39mhead\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m trainer\u001b[39m.\u001b[39minitialize_training()\n\u001b[0;32m---> 20\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain_and_validate()\n",
      "File \u001b[0;32m~/projects/neuralhydrology/neuralhydrology/training/basetrainer.py:215\u001b[0m, in \u001b[0;36mBaseTrainer.train_and_validate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[39mfor\u001b[39;00m param_group \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mparam_groups:\n\u001b[1;32m    213\u001b[0m         param_group[\u001b[39m\"\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcfg\u001b[39m.\u001b[39mlearning_rate[epoch]\n\u001b[0;32m--> 215\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_epoch(epoch\u001b[39m=\u001b[39;49mepoch)\n\u001b[1;32m    216\u001b[0m avg_losses \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperiment_logger\u001b[39m.\u001b[39msummarise()\n\u001b[1;32m    217\u001b[0m loss_str \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mk\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00mv\u001b[39m:\u001b[39;00m\u001b[39m.5f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m avg_losses\u001b[39m.\u001b[39mitems())\n",
      "File \u001b[0;32m~/projects/neuralhydrology/neuralhydrology/training/basetrainer.py:320\u001b[0m, in \u001b[0;36mBaseTrainer._train_epoch\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m    319\u001b[0m \u001b[39m# get gradients\u001b[39;00m\n\u001b[0;32m--> 320\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    322\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcfg\u001b[39m.\u001b[39mclip_gradient_norm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m     torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mclip_grad_norm_(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mparameters(), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcfg\u001b[39m.\u001b[39mclip_gradient_norm)\n",
      "File \u001b[0;32m~/projects/neuralhydrology/.venv/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m~/projects/neuralhydrology/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "def train_and_evaluate(fold, predictors, config_file):\n",
    "    # Set up configuration\n",
    "    cfg = Config(config_file)\n",
    "    cfg.update_config({\n",
    "        'dynamic_inputs': get_predictor_list(predictors),\n",
    "        'train_basin_file': f'basins/twofoldsplit/fold_{fold}_train.txt',\n",
    "        'test_basin_file': f'basins/twofoldsplit/fold_{fold}_test.txt',\n",
    "        'validation_basin_file': f'basins/twofoldsplit/fold_{fold}_test.txt',\n",
    "        'experiment_name': f'spatial_twofold_{fold}_{predictors}',\n",
    "    })\n",
    "\n",
    "    # Set device\n",
    "    cfg.device = f\"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # Start training\n",
    "    start_training(cfg)\n",
    "\n",
    "    # Get the latest run directory\n",
    "    run_dir = get_latest_touched_directory('runs')\n",
    "\n",
    "    # Evaluate on test set\n",
    "    ote = eval_run(run_dir=run_dir, period='test')\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    ova = eval_run(run_dir=run_dir, period='validation')\n",
    "\n",
    "    # Evaluate on train set\n",
    "    otr = eval_run(run_dir=run_dir, period='train')\n",
    "\n",
    "    return ote, ova, otr\n",
    "\n",
    "\n",
    "config_file = Path('configs/config.yml')\n",
    "\n",
    "\n",
    "for predictors in ['p', 'pe', 'e']:\n",
    "    for fold in [0, 1]:\n",
    "        _ = train_and_evaluate(fold, predictors, config_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
