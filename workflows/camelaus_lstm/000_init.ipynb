{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralhydrology.nh_run import start_run, eval_run, continue_run\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from neuralhydrology.utils.file_system_operation import get_latest_touched_directory\n",
    "from neuralhydrology.training.train import start_training\n",
    "from neuralhydrology.utils.config import Config\n",
    "\n",
    "def get_predictor_list(s):\n",
    "    perdictor_dict = {\n",
    "    'p' : [\"precipitation_AWAP\"],\n",
    "    'e' : [\"et_morton_point_SILO\"],\n",
    "    'pe': [\"precipitation_AWAP\", \"et_morton_point_SILO\"]\n",
    "    }\n",
    "    return perdictor_dict[s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-25 09:19:40,120: Logging to /Users/sho108/projects/neuralhydrology/workflows/camelaus_lstm/runs/spatial_twofold_1_p_2501_091940/output.log initialized.\n",
      "2024-01-25 09:19:40,120: ### Folder structure created at /Users/sho108/projects/neuralhydrology/workflows/camelaus_lstm/runs/spatial_twofold_1_p_2501_091940\n",
      "2024-01-25 09:19:40,121: ### Run configurations for spatial_twofold_1_p\n",
      "2024-01-25 09:19:40,121: experiment_name: spatial_twofold_1_p\n",
      "2024-01-25 09:19:40,122: train_basin_file: basins/twofoldsplit/fold_1_train.txt\n",
      "2024-01-25 09:19:40,122: validation_basin_file: basins/twofoldsplit/fold_1_test.txt\n",
      "2024-01-25 09:19:40,123: test_basin_file: basins/twofoldsplit/fold_1_test.txt\n",
      "2024-01-25 09:19:40,123: train_start_date: 1981-01-01 00:00:00\n",
      "2024-01-25 09:19:40,123: train_end_date: 2018-12-31 00:00:00\n",
      "2024-01-25 09:19:40,124: validation_start_date: 1981-01-01 00:00:00\n",
      "2024-01-25 09:19:40,124: validation_end_date: 2018-12-31 00:00:00\n",
      "2024-01-25 09:19:40,124: test_start_date: 1981-01-01 00:00:00\n",
      "2024-01-25 09:19:40,125: test_end_date: 2018-12-31 00:00:00\n",
      "2024-01-25 09:19:40,125: device: cpu\n",
      "2024-01-25 09:19:40,125: validate_every: 5\n",
      "2024-01-25 09:19:40,126: validate_n_random_basins: 10\n",
      "2024-01-25 09:19:40,126: metrics: ['NSE', 'MSE', 'RMSE', 'KGE', 'Alpha-NSE', 'Pearson-r', 'Beta-KGE', 'Beta-NSE', 'FHV', 'FMS', 'FLV', 'Peak-Timing', 'Missed-Peaks', 'Peak-MAPE']\n",
      "2024-01-25 09:19:40,126: model: cudalstm\n",
      "2024-01-25 09:19:40,126: head: regression\n",
      "2024-01-25 09:19:40,127: output_activation: linear\n",
      "2024-01-25 09:19:40,127: hidden_size: 20\n",
      "2024-01-25 09:19:40,128: initial_forget_bias: 3\n",
      "2024-01-25 09:19:40,128: output_dropout: 0.4\n",
      "2024-01-25 09:19:40,129: optimizer: Adam\n",
      "2024-01-25 09:19:40,130: loss: MSE\n",
      "2024-01-25 09:19:40,130: learning_rate: {0: 0.01, 15: 0.005}\n",
      "2024-01-25 09:19:40,130: batch_size: 1024\n",
      "2024-01-25 09:19:40,131: epochs: 30\n",
      "2024-01-25 09:19:40,131: clip_gradient_norm: 1\n",
      "2024-01-25 09:19:40,131: predict_last_n: 1\n",
      "2024-01-25 09:19:40,132: seq_length: 365\n",
      "2024-01-25 09:19:40,132: num_workers: 8\n",
      "2024-01-25 09:19:40,132: log_interval: 5\n",
      "2024-01-25 09:19:40,132: log_tensorboard: True\n",
      "2024-01-25 09:19:40,133: log_n_figures: 1\n",
      "2024-01-25 09:19:40,133: save_weights_every: 1\n",
      "2024-01-25 09:19:40,133: dataset: camels_aus\n",
      "2024-01-25 09:19:40,134: data_dir: /Users/sho108/Desktop/z/Data/CAMELS_AUS\n",
      "2024-01-25 09:19:40,134: dynamic_inputs: ['precipitation_AWAP']\n",
      "2024-01-25 09:19:40,135: target_variables: ['streamflow_mmd']\n",
      "2024-01-25 09:19:40,135: clip_targets_to_zero: ['streamflow_mmd']\n",
      "2024-01-25 09:19:40,135: number_of_basins: 111\n",
      "2024-01-25 09:19:40,136: run_dir: /Users/sho108/projects/neuralhydrology/workflows/camelaus_lstm/runs/spatial_twofold_1_p_2501_091940\n",
      "2024-01-25 09:19:40,136: train_dir: /Users/sho108/projects/neuralhydrology/workflows/camelaus_lstm/runs/spatial_twofold_1_p_2501_091940/train_data\n",
      "2024-01-25 09:19:40,137: img_log_dir: /Users/sho108/projects/neuralhydrology/workflows/camelaus_lstm/runs/spatial_twofold_1_p_2501_091940/img_log\n",
      "2024-01-25 09:19:40,139: ### Device cpu will be used for training\n",
      "2024-01-25 09:19:40,140: Loading basin data into xarray data set.\n",
      "100%|██████████| 111/111 [04:33<00:00,  2.47s/it]\n",
      "2024-01-25 09:24:13,828: Create lookup table and convert to pytorch tensor\n",
      "100%|██████████| 111/111 [00:05<00:00, 20.52it/s]\n",
      "# Epoch 1: 100%|██████████| 1302/1302 [10:16<00:00,  2.11it/s, Loss: 0.1225]\n",
      "2024-01-25 09:34:36,402: Epoch 1 average loss: avg_loss: 0.24155, avg_total_loss: 0.24155\n",
      "# Epoch 2: 100%|██████████| 1302/1302 [10:36<00:00,  2.05it/s, Loss: 0.1823]\n",
      "2024-01-25 09:45:12,725: Epoch 2 average loss: avg_loss: 0.20503, avg_total_loss: 0.20503\n",
      "# Epoch 3: 100%|██████████| 1302/1302 [10:40<00:00,  2.03it/s, Loss: 0.1571]\n",
      "2024-01-25 09:55:53,412: Epoch 3 average loss: avg_loss: 0.20130, avg_total_loss: 0.20130\n",
      "# Epoch 4: 100%|██████████| 1302/1302 [10:35<00:00,  2.05it/s, Loss: 0.1428] \n",
      "2024-01-25 10:06:29,109: Epoch 4 average loss: avg_loss: 0.20221, avg_total_loss: 0.20221\n",
      "# Epoch 5: 100%|██████████| 1302/1302 [10:35<00:00,  2.05it/s, Loss: 0.2165]\n",
      "2024-01-25 10:17:05,034: Epoch 5 average loss: avg_loss: 0.19981, avg_total_loss: 0.19981\n",
      "# Validation: 100%|██████████| 10/10 [01:12<00:00,  7.24s/it]\n",
      "2024-01-25 10:18:17,869: Epoch 5 average validation loss: 0.18420 -- Median validation metrics: avg_loss: 0.18420, NSE: 0.32103, MSE: 1.13149, RMSE: 1.06353, KGE: -0.01748, Alpha-NSE: 1.13711, Pearson-r: 0.77840, Beta-KGE: 1.85429, Beta-NSE: 0.13921, FHV: 12.03093, FMS: 32.86532, FLV: 0.00000, Peak-Timing: 1.02273, Missed-Peaks: 0.52775, Peak-MAPE: 64.54843\n",
      "# Epoch 6: 100%|██████████| 1302/1302 [10:43<00:00,  2.02it/s, Loss: 0.2525] \n",
      "2024-01-25 10:29:01,725: Epoch 6 average loss: avg_loss: 0.19622, avg_total_loss: 0.19622\n",
      "# Epoch 7:  35%|███▌      | 460/1302 [04:19<07:54,  1.77it/s, Loss: 0.1353] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 41\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[39mfor\u001b[39;00m predictors \u001b[39min\u001b[39;00m [\u001b[39m'\u001b[39m\u001b[39mp\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mpe\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39me\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[1;32m     40\u001b[0m     \u001b[39mfor\u001b[39;00m fold \u001b[39min\u001b[39;00m [\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m]:\n\u001b[0;32m---> 41\u001b[0m         _ \u001b[39m=\u001b[39m train_and_evaluate(fold, predictors, config_file)\n",
      "Cell \u001b[0;32mIn[4], line 19\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(fold, predictors, config_file)\u001b[0m\n\u001b[1;32m     16\u001b[0m cfg\u001b[39m.\u001b[39mdevice \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcuda:0\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     18\u001b[0m \u001b[39m# Start training\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m start_training(cfg)\n\u001b[1;32m     21\u001b[0m \u001b[39m# Get the latest run directory\u001b[39;00m\n\u001b[1;32m     22\u001b[0m run_dir \u001b[39m=\u001b[39m get_latest_touched_directory(\u001b[39m'\u001b[39m\u001b[39mruns\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/projects/neuralhydrology/neuralhydrology/training/train.py:20\u001b[0m, in \u001b[0;36mstart_training\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnknown head \u001b[39m\u001b[39m{\u001b[39;00mcfg\u001b[39m.\u001b[39mhead\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m trainer\u001b[39m.\u001b[39minitialize_training()\n\u001b[0;32m---> 20\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain_and_validate()\n",
      "File \u001b[0;32m~/projects/neuralhydrology/neuralhydrology/training/basetrainer.py:215\u001b[0m, in \u001b[0;36mBaseTrainer.train_and_validate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[39mfor\u001b[39;00m param_group \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mparam_groups:\n\u001b[1;32m    213\u001b[0m         param_group[\u001b[39m\"\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcfg\u001b[39m.\u001b[39mlearning_rate[epoch]\n\u001b[0;32m--> 215\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_epoch(epoch\u001b[39m=\u001b[39;49mepoch)\n\u001b[1;32m    216\u001b[0m avg_losses \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperiment_logger\u001b[39m.\u001b[39msummarise()\n\u001b[1;32m    217\u001b[0m loss_str \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mk\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00mv\u001b[39m:\u001b[39;00m\u001b[39m.5f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m avg_losses\u001b[39m.\u001b[39mitems())\n",
      "File \u001b[0;32m~/projects/neuralhydrology/neuralhydrology/training/basetrainer.py:320\u001b[0m, in \u001b[0;36mBaseTrainer._train_epoch\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m    319\u001b[0m \u001b[39m# get gradients\u001b[39;00m\n\u001b[0;32m--> 320\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    322\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcfg\u001b[39m.\u001b[39mclip_gradient_norm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m     torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mclip_grad_norm_(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mparameters(), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcfg\u001b[39m.\u001b[39mclip_gradient_norm)\n",
      "File \u001b[0;32m~/projects/neuralhydrology/.venv/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m~/projects/neuralhydrology/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "def train_and_evaluate(fold, predictors, config_file):\n",
    "    # Set up configuration\n",
    "    cfg = Config(config_file)\n",
    "    cfg.update_config({\n",
    "        'dynamic_inputs': get_predictor_list(predictors),\n",
    "        'train_basin_file': f'basins/twofoldsplit/fold_{fold}_train.txt',\n",
    "        'test_basin_file': f'basins/twofoldsplit/fold_{fold}_test.txt',\n",
    "        'validation_basin_file': f'basins/twofoldsplit/fold_{fold}_test.txt',\n",
    "        'experiment_name': f'spatial_twofold_{fold}_{predictors}',\n",
    "    })\n",
    "\n",
    "    # Set device\n",
    "    cfg.device = f\"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # Start training\n",
    "    start_training(cfg)\n",
    "\n",
    "    # Get the latest run directory\n",
    "    run_dir = get_latest_touched_directory('runs')\n",
    "\n",
    "    # Evaluate on test set\n",
    "    ote = eval_run(run_dir=run_dir, period='test')\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    ova = eval_run(run_dir=run_dir, period='validation')\n",
    "\n",
    "    # Evaluate on train set\n",
    "    otr = eval_run(run_dir=run_dir, period='train')\n",
    "\n",
    "    return ote, ova, otr\n",
    "\n",
    "\n",
    "config_file = Path('configs/config.yml')\n",
    "\n",
    "\n",
    "for predictors in ['p', 'pe', 'e']:\n",
    "    for fold in [1, 0]:\n",
    "        _ = train_and_evaluate(fold, predictors, config_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
